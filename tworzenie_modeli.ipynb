{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-19T14:58:58.182096Z",
     "start_time": "2025-11-19T14:58:58.153690Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "possible_paths = [\n",
    "    \"data_scaled/insurance_cleaned_knn_minmax_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_knn_standard_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_mean_median_minmax_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_mean_median_standard_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_regression_minmax_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_regression_standard_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_sklearn_impute_minmax_scaled.csv\",\n",
    "    \"data_scaled/insurance_cleaned_sklearn_impute_standard_scaled.csv\"\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in possible_paths:\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p)\n",
    "        print(f\"Loaded data from: {p}\")\n",
    "        break\n",
    "\n",
    "target = 'charges'\n",
    "\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Proste kodowanie zmiennych kategorycznych\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# %%\n",
    "# Wykonaj podział na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Podział na train/test wykonany:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# %%\n",
    "# Zapisz splity do katalogu data_converted/ (tworzymy katalog jeśli nie istnieje)\n",
    "out_dir = 'data_splits'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "X_train.to_csv(os.path.join(out_dir, 'X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(out_dir, 'X_test.csv'), index=False)\n",
    "\n",
    "y_train.to_csv(os.path.join(out_dir, 'y_train.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(out_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "print(f\"Zapisano splity w: {out_dir}/\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from: data_scaled/insurance_cleaned_knn_minmax_scaled.csv\n",
      "Podział na train/test wykonany:\n",
      "X_train: (1068, 8), X_test: (267, 8)\n",
      "y_train: (1068,), y_test: (267,)\n",
      "Zapisano splity w: data_splits/\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:04:20.575295Z",
     "start_time": "2025-11-19T15:04:14.656023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "out_dir = 'data_splits'\n",
    "if 'X_train' not in globals() or 'X_test' not in globals():\n",
    "    required = [os.path.join(out_dir, n) for n in ('X_train.csv','X_test.csv','y_train.csv','y_test.csv')]\n",
    "    if not all(os.path.exists(p) for p in required):\n",
    "        raise FileNotFoundError(f\"Brakuje plików w: {out_dir}/ — oczekiwane: {required}\")\n",
    "    X_train = pd.read_csv(required[0])\n",
    "    X_test  = pd.read_csv(required[1])\n",
    "    y_train = pd.read_csv(required[2]).squeeze()\n",
    "    y_test  = pd.read_csv(required[3]).squeeze()\n",
    "\n",
    "# Przygotuj katalog do zapisu modeli\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Definicja modeli scikit-learn\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dodaj modele zewnętrzne jeśli dostępne\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    models[\"XGBoost\"] = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbosity=0)\n",
    "except Exception:\n",
    "    print(\"XGBoost nie jest zainstalowany — pomijam.\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    models[\"LightGBM\"] = LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "except Exception:\n",
    "    print(\"LightGBM nie jest zainstalowany — pomijam.\")\n",
    "\n",
    "# Trening, ewaluacja i zapis\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        results[name] = {\"rmse\": float(rmse), \"r2\": float(r2)}\n",
    "        joblib.dump(model, os.path.join(models_dir, f\"{name}.joblib\"))\n",
    "        print(f\"{name}: RMSE={rmse:.4f}, R2={r2:.4f}  -> saved to {models_dir}/{name}.joblib\")\n",
    "    except Exception as e:\n",
    "        print(f\"Nie udało się wytrenować/zapisać modelu {name}: {e}\")\n",
    "\n",
    "# Podsumowanie wyników\n",
    "print(\"\\nPodsumowanie wyników:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:15s}  RMSE={v['rmse']:.4f}  R2={v['r2']:.4f}\")\n",
    "# %%"
   ],
   "id": "85e103fd0422b3f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: RMSE=7010.6536, R2=0.6884  -> saved to models/LinearRegression.joblib\n",
      "Ridge: RMSE=7020.6311, R2=0.6875  -> saved to models/Ridge.joblib\n",
      "RandomForest: RMSE=6408.6460, R2=0.7396  -> saved to models/RandomForest.joblib\n",
      "GradientBoosting: RMSE=6262.3726, R2=0.7514  -> saved to models/GradientBoosting.joblib\n",
      "XGBoost: RMSE=6961.5769, R2=0.6928  -> saved to models/XGBoost.joblib\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 1068, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 12693.808952\n",
      "LightGBM: RMSE=6564.9821, R2=0.7268  -> saved to models/LightGBM.joblib\n",
      "\n",
      "Podsumowanie wyników:\n",
      "LinearRegression  RMSE=7010.6536  R2=0.6884\n",
      "Ridge            RMSE=7020.6311  R2=0.6875\n",
      "RandomForest     RMSE=6408.6460  R2=0.7396\n",
      "GradientBoosting  RMSE=6262.3726  R2=0.7514\n",
      "XGBoost          RMSE=6961.5769  R2=0.6928\n",
      "LightGBM         RMSE=6564.9821  R2=0.7268\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:12:33.024316Z",
     "start_time": "2025-11-19T15:12:23.872269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "\n",
    "estimators = [\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "    (\"gbr\", GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "    (\"lr\", LinearRegression()),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42)),\n",
    "    (\"xgb\", XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1, verbosity=0)),\n",
    "    (\"lgbm\", LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "# VotingRegressor\n",
    "try:\n",
    "    voting = VotingRegressor(estimators=estimators, n_jobs=-1)\n",
    "    voting.fit(X_train, y_train)\n",
    "    preds_v = voting.predict(X_test)\n",
    "    rmse_v = np.sqrt(mean_squared_error(y_test, preds_v))\n",
    "    r2_v = r2_score(y_test, preds_v)\n",
    "    results[\"VotingRegressor\"] = {\"rmse\": float(rmse_v), \"r2\": float(r2_v)}\n",
    "    joblib.dump(voting, os.path.join(models_dir, \"VotingRegressor.joblib\"))\n",
    "    print(f\"VotingRegressor: RMSE={rmse_v:.4f}, R2={r2_v:.4f} -> saved to {models_dir}/VotingRegressor.joblib\")\n",
    "except Exception as e:\n",
    "    print(f\"Nie udało się wytrenować/zapisać VotingRegressor: {e}\")\n",
    "\n",
    "# StackingRegressor\n",
    "try:\n",
    "    stacking = StackingRegressor(estimators=estimators, final_estimator=Ridge(random_state=42), n_jobs=-1, passthrough=False)\n",
    "    stacking.fit(X_train, y_train)\n",
    "    preds_s = stacking.predict(X_test)\n",
    "    rmse_s = np.sqrt(mean_squared_error(y_test, preds_s))\n",
    "    r2_s = r2_score(y_test, preds_s)\n",
    "    results[\"StackingRegressor\"] = {\"rmse\": float(rmse_s), \"r2\": float(r2_s)}\n",
    "    joblib.dump(stacking, os.path.join(models_dir, \"StackingRegressor.joblib\"))\n",
    "    print(f\"StackingRegressor: RMSE={rmse_s:.4f}, R2={r2_s:.4f} -> saved to {models_dir}/StackingRegressor.joblib\")\n",
    "except Exception as e:\n",
    "    print(f\"Nie udało się wytrenować/zapisać StackingRegressor: {e}\")\n",
    "\n",
    "# Podsumowanie wyników\n",
    "print(\"\\nPodsumowanie wyników ensemble:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:20s}  RMSE={v['rmse']:.4f}  R2={v['r2']:.4f}\")"
   ],
   "id": "bb85034e0a3f2321",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingRegressor: RMSE=6350.8568, R2=0.7443 -> saved to models/VotingRegressor.joblib\n",
      "StackingRegressor: RMSE=6297.8466, R2=0.7486 -> saved to models/StackingRegressor.joblib\n",
      "\n",
      "Podsumowanie wyników ensemble:\n",
      "LinearRegression      RMSE=7010.6536  R2=0.6884\n",
      "Ridge                 RMSE=7020.6311  R2=0.6875\n",
      "RandomForest          RMSE=6408.6460  R2=0.7396\n",
      "GradientBoosting      RMSE=6262.3726  R2=0.7514\n",
      "XGBoost               RMSE=6961.5769  R2=0.6928\n",
      "LightGBM              RMSE=6564.9821  R2=0.7268\n",
      "VotingRegressor       RMSE=6350.8568  R2=0.7443\n",
      "StackingRegressor     RMSE=6297.8466  R2=0.7486\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:14:41.532364Z",
     "start_time": "2025-11-19T15:14:41.457552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Wczytaj dane testowe ---\n",
    "out_dir = 'data_splits'\n",
    "required = [os.path.join(out_dir, n) for n in ('X_test.csv','y_test.csv')]\n",
    "if not all(os.path.exists(p) for p in required):\n",
    "    raise FileNotFoundError(f\"Brakuje plików w: {out_dir}/ — oczekiwane: {required}\")\n",
    "\n",
    "X_test = pd.read_csv(required[0])\n",
    "y_test = pd.read_csv(required[1]).squeeze().to_numpy()\n",
    "\n",
    "# --- Definicje metryk (ręcznie) ---\n",
    "def mse(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "    return np.mean(err**2)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def median_ae(y_true, y_pred):\n",
    "    return np.median(np.abs(y_true - y_pred))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    # bezdzielnik przy zerach - pomijamy elementy z y_true == 0\n",
    "    denom = np.where(y_true == 0, np.nan, y_true)\n",
    "    pct_errors = np.abs((y_true - y_pred) / denom)\n",
    "    # jeśli wszystkie y_true==0 zwróć nan\n",
    "    return np.nanmean(pct_errors) * 100.0\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
    "\n",
    "def explained_variance(y_true, y_pred):\n",
    "    var_res = np.var(y_true - y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    return 1.0 - var_res / var_true if var_true != 0 else np.nan\n",
    "\n",
    "# --- Zbierz modele: użyj istniejącej zmiennej 'models' jeśli jest, inaczej załaduj .joblib z katalogu models/ ---\n",
    "models_dict = {}\n",
    "if 'models' in globals() and isinstance(models, dict):\n",
    "    # jeżeli wartości to obiekty modeli\n",
    "    for name, mod in models.items():\n",
    "        try:\n",
    "            # jeśli model nie jest wytrenowany, predict może rzucić; obsłużymy wyjątek później\n",
    "            models_dict[name] = mod\n",
    "        except Exception:\n",
    "            pass\n",
    "else:\n",
    "    models_dir = 'models'\n",
    "    if os.path.exists(models_dir):\n",
    "        for f in os.listdir(models_dir):\n",
    "            if f.endswith('.joblib') or f.endswith('.pkl'):\n",
    "                name = os.path.splitext(f)[0]\n",
    "                try:\n",
    "                    models_dict[name] = joblib.load(os.path.join(models_dir, f))\n",
    "                except Exception as e:\n",
    "                    print(f\"Nie udało się załadować modelu {f}: {e}\")\n",
    "\n",
    "if not models_dict:\n",
    "    raise RuntimeError(\"Brak dostępnych modeli do oceny (brak zmiennej `models` i brak plików w `models/`).\")\n",
    "\n",
    "# --- Ewaluacja ---\n",
    "rows = []\n",
    "for name, model in models_dict.items():\n",
    "    try:\n",
    "        preds = model.predict(X_test)\n",
    "        preds = np.array(preds).squeeze()\n",
    "        y_true = np.array(y_test).squeeze()\n",
    "        # ujednolicenie długości\n",
    "        if preds.shape != y_true.shape:\n",
    "            print(f\"Model {name} - niespójne kształty pred/y: {preds.shape} vs {y_true.shape}, pomijam.\")\n",
    "            continue\n",
    "        vals = {\n",
    "            \"model\": name,\n",
    "            \"n_samples\": int(y_true.size),\n",
    "            \"RMSE\": float(rmse(y_true, preds)),\n",
    "            \"MAE\": float(mae(y_true, preds)),\n",
    "            \"Median_AE\": float(median_ae(y_true, preds)),\n",
    "            \"MAPE_%\": float(mape(y_true, preds)) if not np.isnan(mape(y_true, preds)) else np.nan,\n",
    "            \"R2\": float(r2_score_manual(y_true, preds)),\n",
    "            \"ExplainedVar\": float(explained_variance(y_true, preds))\n",
    "        }\n",
    "        rows.append(vals)\n",
    "    except Exception as e:\n",
    "        print(f\"Nie udało się ocenić modelu {name}: {e}\")\n",
    "\n",
    "# --- Wyniki: DataFrame, wydruk i zapis ---\n",
    "metrics_df = pd.DataFrame(rows).sort_values(by='RMSE')\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "os.makedirs('metrics', exist_ok=True)\n",
    "metrics_df.to_csv(os.path.join('metrics', 'metrics.csv'), index=False)\n",
    "print(f\"\\nZapisano metryki do: metrics/metrics.csv\")"
   ],
   "id": "922e643204a20577",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model  n_samples        RMSE         MAE   Median_AE    MAPE_%       R2  ExplainedVar\n",
      "GradientBoosting        267 6262.372562 4041.655409 2453.214910 47.651951 0.751389      0.751769\n",
      "    RandomForest        267 6408.646028 4192.134573 2461.138272 47.204999 0.739640      0.739649\n",
      "        LightGBM        267 6564.982083 4297.189138 2816.928451 49.104254 0.726782      0.728068\n",
      "         XGBoost        267 6961.576888 4570.305830 2807.239758 53.659345 0.692774      0.692856\n",
      "LinearRegression        267 7010.653603 4943.082393 3129.907520 50.386786 0.688427      0.689017\n",
      "           Ridge        267 7020.631104 4950.449881 3099.069207 50.642668 0.687540      0.688158\n",
      "\n",
      "Zapisano metryki do: metrics/metrics.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T15:17:43.286176Z",
     "start_time": "2025-11-19T15:17:41.470416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(style=\"whitegrid\", rc={\"figure.figsize\": (8, 5)})\n",
    "\n",
    "# --- ustawienia ścieżek ---\n",
    "metrics_fp = os.path.join('metrics', 'metrics.csv')\n",
    "models_dir = 'models'\n",
    "reports_dir = 'reports'\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# --- wczytaj testowe dane ---\n",
    "out_dir = 'data_splits'\n",
    "X_test = pd.read_csv(os.path.join(out_dir, 'X_test.csv'))\n",
    "y_test = pd.read_csv(os.path.join(out_dir, 'y_test.csv')).squeeze().to_numpy()\n",
    "\n",
    "# --- wczytaj tabelę metryk jeśli istnieje ---\n",
    "if os.path.exists(metrics_fp):\n",
    "    metrics_df = pd.read_csv(metrics_fp)\n",
    "else:\n",
    "    metrics_df = None\n",
    "\n",
    "# --- załaduj modele (preferuj zmienną `models` jeśli jest) ---\n",
    "models_dict = {}\n",
    "if 'models' in globals() and isinstance(models, dict) and models:\n",
    "    models_dict = models.copy()\n",
    "else:\n",
    "    if os.path.exists(models_dir):\n",
    "        for f in os.listdir(models_dir):\n",
    "            if f.endswith('.joblib') or f.endswith('.pkl'):\n",
    "                name = os.path.splitext(f)[0]\n",
    "                try:\n",
    "                    models_dict[name] = joblib.load(os.path.join(models_dir, f))\n",
    "                except Exception as e:\n",
    "                    print(f\"Nie udało się załadować {f}: {e}\")\n",
    "\n",
    "if not models_dict:\n",
    "    raise RuntimeError(\"Brak modeli w `models` ani w katalogu `models/`.\")\n",
    "\n",
    "# --- oblicz dodatkowe statystyki i reszty ---\n",
    "rows = []\n",
    "preds_all = {}\n",
    "for name, model in models_dict.items():\n",
    "    try:\n",
    "        preds = np.array(model.predict(X_test)).squeeze()\n",
    "    except Exception as e:\n",
    "        print(f\"Predict error {name}: {e}\")\n",
    "        continue\n",
    "    preds_all[name] = preds\n",
    "    err = y_test - preds\n",
    "    abs_err = np.abs(err)\n",
    "    mse = np.mean(err**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(abs_err)\n",
    "    median_ae = np.median(abs_err)\n",
    "    # MAPE: pomijamy y_true == 0\n",
    "    denom = np.where(y_test == 0, np.nan, y_test)\n",
    "    mape = np.nanmean(np.abs((y_test - preds) / denom)) * 100.0\n",
    "    ss_res = np.sum(err**2)\n",
    "    ss_tot = np.sum((y_test - np.mean(y_test))**2)\n",
    "    r2 = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
    "    explained = 1 - np.var(err) / np.var(y_test) if np.var(y_test) != 0 else np.nan\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"n_samples\": int(y_test.size),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"MAE\": float(mae),\n",
    "        \"Median_AE\": float(median_ae),\n",
    "        \"MAPE_%\": float(mape) if not np.isnan(mape) else np.nan,\n",
    "        \"R2\": float(r2),\n",
    "        \"ExplainedVar\": float(explained)\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(rows).sort_values(by='RMSE').reset_index(drop=True)\n",
    "print(\"\\nTabela porównawcza (posortowane po RMSE):\")\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# zapisz uaktualnione metryki\n",
    "os.makedirs('metrics', exist_ok=True)\n",
    "comp_df.to_csv(os.path.join('metrics', 'metrics_full.csv'), index=False)\n",
    "\n",
    "# --- wykres 1: słupek RMSE i R2 ---\n",
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(x='RMSE', y='model', data=comp_df, palette='mako', ax=ax1)\n",
    "ax1.set_title('RMSE (niższe = lepsze)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(reports_dir, 'rmse_bar.png'))\n",
    "plt.close(fig)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(x='R2', y='model', data=comp_df, palette='crest', ax=ax)\n",
    "ax.set_title('R2 (wyższe = lepsze)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(reports_dir, 'r2_bar.png'))\n",
    "plt.close(fig)\n",
    "\n",
    "# --- wybierz top2 do dalszych analiz ---\n",
    "top_models = comp_df['model'].tolist()[:2]\n",
    "print(f\"\\nTop2 modele do dalszej analizy: {top_models}\")\n",
    "\n",
    "# --- wykres 2: scatter y_true vs y_pred dla top3 (jeśli jest) ---\n",
    "top3 = comp_df['model'].tolist()[:3]\n",
    "fig, axes = plt.subplots(1, len(top3), figsize=(5*len(top3),5))\n",
    "if len(top3) == 1:\n",
    "    axes = [axes]\n",
    "for ax, name in zip(axes, top3):\n",
    "    preds = preds_all[name]\n",
    "    sns.scatterplot(x=y_test, y=preds, alpha=0.6, ax=ax)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    ax.set_title(f\"{name}\\nRMSE={comp_df.loc[comp_df.model==name,'RMSE'].values[0]:.0f}\")\n",
    "    ax.set_xlabel(\"y_true\")\n",
    "    ax.set_ylabel(\"y_pred\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(reports_dir, 'ytrue_ypred_top3.png'))\n",
    "plt.close(fig)\n",
    "\n",
    "# --- wykres 3: histogram reszt i wykres reszt vs pred dla najlepszego modelu ---\n",
    "best = top_models[0]\n",
    "best_preds = preds_all[best]\n",
    "resid = y_test - best_preds\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.histplot(resid, bins=30, kde=True, ax=axs[0])\n",
    "axs[0].set_title(f\"Histogram reszt: {best}\")\n",
    "sns.scatterplot(x=best_preds, y=resid, alpha=0.6, ax=axs[1])\n",
    "axs[1].axhline(0, color='r', linestyle='--')\n",
    "axs[1].set_title(f\"Reszty vs przewidywane: {best}\")\n",
    "axs[1].set_xlabel(\"y_pred\")\n",
    "axs[1].set_ylabel(\"resid\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(reports_dir, 'resid_best.png'))\n",
    "plt.close(fig)\n",
    "\n",
    "# --- wykres 4: boxplot błędów bezwzględnych wszystkich modeli ---\n",
    "ae_df = pd.DataFrame({name: np.abs(preds_all[name] - y_test) for name in preds_all.keys()})\n",
    "ae_melt = ae_df.melt(var_name='model', value_name='abs_error')\n",
    "plt.figure(figsize=(10,6))\n",
    "order = comp_df['model'].tolist()\n",
    "sns.boxplot(x='abs_error', y='model', data=ae_melt, order=order, palette='vlag')\n",
    "plt.title('Boxplot błędów bezwzględnych (porównanie modeli)')\n",
    "plt.xlabel('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(reports_dir, 'abs_error_boxplot.png'))\n",
    "plt.close()\n",
    "\n",
    "# --- test parowy (ttest_rel) między top2 modeli na absolutnych błędach ---\n",
    "if len(top_models) >= 2:\n",
    "    a = np.abs(preds_all[top_models[0]] - y_test)\n",
    "    b = np.abs(preds_all[top_models[1]] - y_test)\n",
    "    # sprawdź zgodność długości:\n",
    "    if a.shape == b.shape:\n",
    "        tstat, pval = stats.ttest_rel(a, b, nan_policy='omit')\n",
    "        mean_diff = np.mean(a - b)\n",
    "        print(f\"\\nPaired t-test (abs errors) {top_models[0]} vs {top_models[1]}:\")\n",
    "        print(f\" t = {tstat:.3f}, p = {pval:.3e}, mean(abs_err_diff) = {mean_diff:.3f} ({'+' if mean_diff>0 else '-'})\")\n",
    "        with open(os.path.join(reports_dir, 'paired_ttest.txt'), 'w') as f:\n",
    "            f.write(f\"Paired t-test {top_models[0]} vs {top_models[1]}:\\n t = {tstat:.6f}\\n p = {pval:.6e}\\n mean_abs_err_diff = {mean_diff:.6f}\\n\")\n",
    "    else:\n",
    "        print(\"Top2 modele mają różne kształty predykcji — nie można przeprowadzić t-testu.\")\n",
    "else:\n",
    "    print(\"Za mało modeli do testu parowego.\")\n",
    "\n",
    "print(f\"\\nWykresy i pliki raportów zapisano w: {reports_dir}/  (pliki: rmse_bar.png, r2_bar.png, ytrue_ypred_top3.png, resid_best.png, abs_error_boxplot.png, paired_ttest.txt jeśli dostępny)\")"
   ],
   "id": "4f97a2a57694b087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabela porównawcza (posortowane po RMSE):\n",
      "           model  n_samples        RMSE         MAE   Median_AE    MAPE_%       R2  ExplainedVar\n",
      "GradientBoosting        267 6262.372562 4041.655409 2453.214910 47.651951 0.751389      0.751769\n",
      "    RandomForest        267 6408.646028 4192.134573 2461.138272 47.204999 0.739640      0.739649\n",
      "        LightGBM        267 6564.982083 4297.189138 2816.928451 49.104254 0.726782      0.728068\n",
      "         XGBoost        267 6961.576888 4570.305830 2807.239758 53.659345 0.692774      0.692856\n",
      "LinearRegression        267 7010.653603 4943.082393 3129.907520 50.386786 0.688427      0.689017\n",
      "           Ridge        267 7020.631104 4950.449881 3099.069207 50.642668 0.687540      0.688158\n",
      "\n",
      "Top2 modele do dalszej analizy: ['GradientBoosting', 'RandomForest']\n",
      "\n",
      "Paired t-test (abs errors) GradientBoosting vs RandomForest:\n",
      " t = -1.249, p = 2.129e-01, mean(abs_err_diff) = -150.479 (-)\n",
      "\n",
      "Wykresy i pliki raportów zapisano w: reports/  (pliki: rmse_bar.png, r2_bar.png, ytrue_ypred_top3.png, resid_best.png, abs_error_boxplot.png, paired_ttest.txt jeśli dostępny)\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
